---
id: quiz
title: Chapter 11 Quiz - Voice-to-Action Pipeline
sidebar_position: 99
description: Test your understanding of voice-controlled robotics, Whisper ASR, LLMs, and end-to-end pipeline optimization
---

# Chapter 11 Quiz: Voice-to-Action Pipeline

Test your mastery of voice-controlled robot systems!

:::info Quiz Instructions
- 7 questions covering voice recognition, LLM parsing, VLA integration, and pipeline optimization
- Detailed explanations provided for each answer
- Score 80% or higher (6/7) to demonstrate mastery
:::

---

## Question 1: Whisper Model Selection

**You need &lt;3 second end-to-end latency for voice commands. Which Whisper model should you use?**

<details>
<summary>A) Whisper Tiny (39M parameters, ~100ms)</summary>

 **Fastest but too inaccurate**

**Whisper Tiny specs:**
- Latency: ~100ms (excellent!)
- Accuracy: 88% word accuracy (poor for robotics)
- VRAM: 1 GB

**Problem:** 88% accuracy means 1 in 8 words wrong  frequent command failures

**Example errors:**
- User: "pick up the red cup"
- Tiny: "pick up the **dead** cup" (12% error  task fails)

**Better:** Whisper Base (92% accuracy, 200ms) for best speed/accuracy trade-off.
</details>

<details>
<summary>B) Whisper Base (74M parameters, ~200ms)</summary>

 **Correct - Best Choice!**

**Whisper Base is optimal for robotics:**

**Advantages:**
- **Latency:** ~200ms (well within &lt;3s budget)
- **Accuracy:** 92% word accuracy (reliable for commands)
- **VRAM:** 1 GB (leaves room for VLA model)
- **Multilingual:** Supports 99 languages

**Latency breakdown for &lt;3s target:**
- Audio capture: ~50ms
- Whisper Base: ~200ms 
- LLM parsing: ~500-1000ms
- VLA inference: ~100-200ms
- ROS 2 publish: ~30ms
- **Total:** ~880-1480ms  (well under 3000ms)

**Why not larger models:**
- Medium/Large: 800-1500ms (too slow for &lt;3s target)
- Tiny/Small: Lower accuracy  more failures

**Production recommendation:** Whisper Base for robotics applications.
</details>

<details>
<summary>C) Whisper Large (1.5B parameters, ~1500ms)</summary>

L **Too slow**

**Whisper Large specs:**
- Latency: ~1500ms (half the latency budget!)
- Accuracy: 98% (excellent but overkill)
- VRAM: 10 GB (doesn't fit with VLA model)

**Problem:** 1500ms Whisper + 1000ms LLM + 200ms VLA = 2700ms  very tight budget, risks >3s failures

**Better:** Whisper Base (200ms) gives 1300ms more headroom for other pipeline stages.
</details>

<details>
<summary>D) Whisper Medium (769M parameters, ~800ms)</summary>

 **Possible but suboptimal**

**Whisper Medium specs:**
- Latency: ~800ms (acceptable)
- Accuracy: 97% (excellent)
- VRAM: 5 GB

**Trade-off:** Medium accuracy (97%) vs Base (92%) = only 5% improvement
**Cost:** 4x slower (800ms vs 200ms)

**For robotics:** 92% accuracy is sufficient; save 600ms for faster pipeline.

**Better:** Whisper Base unless ultra-high accuracy required (medical, safety-critical).
</details>

---

## Question 2: Pipeline Latency Budget

**Your pipeline latencies: Whisper (200ms), LLM (1200ms), VLA (150ms), ROS 2 (30ms). What's the bottleneck?**

<details>
<summary>A) Whisper ASR (200ms)</summary>

L **Not the bottleneck**

Whisper at 200ms is **only 12.7%** of total latency (1580ms).

**Bottleneck:** LLM parsing at 1200ms (76% of total latency).
</details>

<details>
<summary>B) LLM parsing (1200ms)</summary>

 **Correct - This is the bottleneck!**

**LLM parsing dominates latency:**
- **LLM:** 1200ms (76% of total)
- Whisper: 200ms (13%)
- VLA: 150ms (9%)
- ROS 2: 30ms (2%)
- **Total:** 1580ms

**How to optimize LLM bottleneck:**

1. **Use smaller LLM:**
   - Switch from Llama 2-13B  Llama 2-7B (2x faster)
   - Use quantized INT8 model (1.5-2x faster)

2. **Simplify parsing:**
   - Pre-defined templates instead of full JSON parsing
   - Rule-based parsing for common commands

3. **Cache common commands:**
   - "pick up X"  cached template
   - Only run LLM for novel/complex commands

4. **Parallel processing:**
   - Start LLM parsing while Whisper still transcribing (pipeline parallelism)

**Target:** Reduce LLM from 1200ms  500ms (2.4x speedup)  total latency 880ms
</details>

<details>
<summary>C) VLA inference (150ms)</summary>

L **Not the bottleneck**

VLA at 150ms is only **9.5%** of total latency.

**Already optimized:** INT8 quantization gives 150ms (vs 300ms FP16).

**Bottleneck:** LLM at 1200ms.
</details>

<details>
<summary>D) ROS 2 publishing (30ms)</summary>

L **Not the bottleneck**

ROS 2 at 30ms is only **1.9%** of total latency - already very fast.

**Bottleneck:** LLM at 1200ms (76%).
</details>

---

## Question 3: Error Handling

**Whisper returns confidence 65%. What should the system do?**

<details>
<summary>A) Proceed with command execution anyway</summary>

L **Dangerous!**

Low confidence (65%) means high probability of transcription errors.

**Risk:**
- User says: "place the cup on the table"
- Whisper (65% conf): "**race** the cup **under** the table"
- Robot executes wrong command  potential damage

**Better:** Reject low-confidence transcriptions, ask user to repeat.
</details>

<details>
<summary>B) Reject command and ask user to repeat</summary>

 **Correct - Safe and user-friendly!**

**Best practice for low confidence (&lt;70%):**

```python
text, confidence = whisper.transcribe(audio)

if confidence < 0.70:  # 70% threshold
    print("  Low confidence. Please repeat command.")
    return False  # Don't execute
```

**Why 70% threshold:**
- **&lt;70%:** High error probability  reject
- **70-85%:** Acceptable for non-critical tasks
- **>85%:** High confidence  execute immediately

**User experience:**
```
User: [mumbles] "pick... uh... cup"
System: "  I didn't catch that. Please repeat."
User: [clearly] "pick up the red cup"
System: " Understood: 'pick up the red cup' (confidence: 94%)"
```

**Production systems:** Add retry counter (max 3 attempts) before fallback to manual control.
</details>

<details>
<summary>C) Use the transcription but log a warning</summary>

 **Risky compromise**

Logging helps debugging but doesn't prevent errors from reaching the robot.

**Problem:** Robot still executes potentially wrong command.

**Better:** Reject and ask for repeat (safe), OR use multi-modal confirmation (show transcription on screen, ask user to confirm).
</details>

<details>
<summary>D) Increase microphone volume and retry automatically</summary>

L **Ineffective**

Low confidence usually due to:
- Background noise (increasing volume won't help)
- Unclear speech (volume doesn't improve articulation)
- Out-of-vocabulary words (volume irrelevant)

**Better:** Ask user to speak clearly in quiet environment.
</details>

---

## Question 4: LLM Command Parsing

**Why parse voice commands into structured JSON before sending to VLA?**

<details>
<summary>A) VLA models can't understand natural language directly</summary>

L **Incorrect**

VLA models (OpenVLA, GR00T N1) ARE trained on natural language inputs!

**VLA input:**
```python
image = robot_camera.capture()
text = "pick up the red cup"
actions = vla_model(image, text)  # Natural language works!
```

**So why parse?** Not because VLAs can't handle it, but for other benefits (see correct answer).
</details>

<details>
<summary>B) Structured format enables better error handling and validation</summary>

 **Correct!**

**Structured parsing adds robustness:**

**Without parsing:**
```python
# Raw voice command
text = "uhhh... pick up the... red thing... no wait, blue cup"
actions = vla_model(image, text)  # Confusing input  unpredictable output
```

**With parsing:**
```python
# LLM extracts clean command
parsed = llm_parser(text)
# Output: {"action": "pick_up", "object": "cup", "color": "blue"}

# Validate before VLA
if parsed["action"] not in ALLOWED_ACTIONS:
    return "Error: Unknown action"
if parsed["object"] is None:
    return "Error: No object specified"

# Clean input to VLA
instruction = f"pick up the {parsed['color']} {parsed['object']}"
actions = vla_model(image, instruction)
```

**Benefits:**
1. **Validation:** Catch invalid commands before VLA
2. **Normalization:** Convert variations ("grab", "grasp", "pick up")  standardized "pick_up"
3. **Error messages:** Clear feedback ("No object specified") vs generic VLA failure
4. **Logging:** Structured data easier to analyze than raw text
5. **Multi-step:** Break complex commands into sequence of simple actions

**Example multi-step:**
```
Voice: "pick up the red cup and place it on the table"
LLM parses to:
  Step 1: {"action": "pick_up", "object": "cup", "color": "red"}
  Step 2: {"action": "place", "object": "cup", "destination": "table"}
```
</details>

<details>
<summary>C) JSON format is required by ROS 2</summary>

L **Incorrect**

ROS 2 uses message types (sensor_msgs/JointState), not JSON.

**VLA output  ROS 2:**
```python
actions = vla_model(image, text)  # numpy array

joint_msg = JointState()
joint_msg.position = actions.tolist()  # Convert to list, not JSON
robot.publish(joint_msg)
```

**JSON parsing is for LLM  VLA, not VLA  ROS 2.**
</details>

<details>
<summary>D) It reduces VLA inference time</summary>

L **Incorrect - Actually INCREASES total time**

**Latency comparison:**

**Direct (no parsing):**
- Whisper: 200ms
- VLA: 150ms
- **Total:** 350ms

**With LLM parsing:**
- Whisper: 200ms
- **LLM parsing: +1000ms** 
- VLA: 150ms
- **Total:** 1350ms (slower!)

**Trade-off:** LLM parsing adds latency BUT improves robustness and error handling.

**Optimization:** Use fast LLM (Llama 2-7B INT8) or rule-based parsing for simple commands.
</details>

---

## Question 5: Multimodal Input

**VLA models require both image AND text. What happens if camera fails but voice works?**

<details>
<summary>A) VLA model automatically uses previous frame</summary>

L **Not automatic**

VLAs don't have built-in frame caching. You must implement this explicitly:

```python
class VLAPipeline:
    def __init__(self):
        self.last_valid_image = None

    def generate_actions(self, current_image, text):
        if current_image is None:
            if self.last_valid_image is not None:
                print("  Using cached image from 100ms ago")
                current_image = self.last_valid_image
            else:
                return None  # No image available
        else:
            self.last_valid_image = current_image  # Cache for next iteration

        return vla_model(current_image, text)
```

**Risk:** Stale image data  actions based on outdated scene.
</details>

<details>
<summary>B) System should reject command and alert user</summary>

 **Correct - Safest approach!**

**Best practice when camera fails:**

```python
if current_image is None:
    print("L ERROR: No camera image available")
    print("   Please check camera connection")
    return False  # Don't execute blind commands
```

**Why this is best:**
1. **Safety:** VLA needs visual context to generate safe actions
2. **User feedback:** Clear error message
3. **Debugging:** User knows to check camera, not voice system

**Alternative for non-critical tasks:**
- Use cached frame (with timestamp check)
- Use synthetic/placeholder image for testing

**For humanoid robots:** Visual feedback is CRITICAL for balance and navigation  always reject if no camera.
</details>

<details>
<summary>C) VLA runs in text-only mode</summary>

L **Not possible**

VLA models (OpenVLA, GR00T N1) are trained with BOTH modalities. They can't operate with text alone.

**Architecture:**
```python
vision_features = vision_encoder(image)   # Required
language_features = language_model(text)  # Required
fused = fusion_layer(vision_features, language_features)
actions = action_decoder(fused)
```

**Missing image = missing half the input  model fails or produces nonsense.**

**Text-only robot control:** Use classical NLP + inverse kinematics, not VLA.
</details>

<details>
<summary>D) Robot executes last known successful action</summary>

L **Dangerous**

Repeating previous action without visual context can cause:
- Collisions (environment changed)
- Incorrect grasps (object moved)
- Safety hazards (person entered workspace)

**Better:** Reject command and alert user about camera failure.
</details>

---

## Question 6: Pipeline Optimization

**You want to reduce latency from 2.5s to &lt;2.0s. Which optimization has BIGGEST impact?**

<details>
<summary>A) Switch Whisper Base  Tiny (save 100ms)</summary>

 **Small impact, big accuracy cost**

**Savings:** 200ms  100ms = **-100ms**
**Cost:** 92%  88% accuracy = **-4% accuracy**

**Not worth it:** Accuracy loss causes more failures than 100ms saves.
</details>

<details>
<summary>B) Quantize LLM to INT8 (save ~600ms)</summary>

 **Correct - Biggest Impact!**

**LLM quantization:**
- **FP16:** ~1200ms
- **INT8:** ~600ms
- **Savings: -600ms** (24% total latency reduction!)

**How to implement:**
```python
from transformers import BitsAndBytesConfig

quant_config = BitsAndBytesConfig(load_in_8bit=True)

llm = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-chat-hf",
    quantization_config=quant_config,
    device_map="auto"
)
```

**Result:**
- **Before:** Whisper (200ms) + LLM (1200ms) + VLA (150ms) = **2550ms**
- **After:** Whisper (200ms) + LLM (600ms) + VLA (150ms) = **1950ms**  &lt;2000ms!

**Accuracy impact:** &lt;1% (minimal)
</details>

<details>
<summary>C) Use faster GPU (RTX 4090 vs 4070 Ti)</summary>

 **Expensive, moderate impact**

**Speed improvement:**
- Whisper: 200ms  150ms (-50ms)
- LLM: 1200ms  900ms (-300ms)
- VLA: 150ms  100ms (-50ms)
- **Total savings: ~400ms**

**Cost:** $1600 GPU upgrade

**Better:** INT8 quantization (free, -600ms) then optimize code before buying new hardware.
</details>

<details>
<summary>D) Run Whisper and LLM in parallel</summary>

L **Not possible - they're sequential**

Pipeline stages have dependencies:
1. Whisper outputs text
2. **LLM needs Whisper's text** as input
3. VLA needs LLM's parsed command

**Can't parallelize:** LLM must wait for Whisper to finish.

**What you CAN parallelize:**
- Whisper + VLA model loading (initialization)
- Multiple voice commands in batch (if processing queue)
</details>

---

## Question 7: Production Deployment

**You're deploying voice control in noisy warehouse. What's the MOST important addition?**

<details>
<summary>A) Louder microphone</summary>

L **Makes problem worse**

Louder microphone amplifies BOTH:
- User voice (good)
- Background noise (bad)

**Result:** Even lower signal-to-noise ratio!

**Better:** Noise-canceling microphone or noise suppression model.
</details>

<details>
<summary>B) Wake word detection ("Hey Robot")</summary>

 **Correct - Essential for noisy environments!**

**Wake word prevents false activations:**

**Without wake word:**
- System listens continuously
- Background voices: "pick up that box"  robot activates incorrectly L
- Machine noise: "sshhh-pick-shhh"  spurious transcriptions L

**With wake word:**
```python
# Stage 0: Wake word detection (runs continuously)
audio_stream = microphone.stream()
if wake_word_detector.detect("hey robot", audio_stream):
    # Only NOW start voice command pipeline
    command_audio = record_command(duration=3.0)
    process_pipeline(command_audio)
```

**Benefits:**
1. **False activation prevention:** Only activates when user says "hey robot"
2. **Privacy:** Not constantly transcribing ambient speech
3. **Latency:** Wake word runs on lightweight model (10-50ms), full pipeline only when needed
4. **User control:** Explicit activation signal

**Implementation:**
```python
from openwakeword import Model

wake_model = Model(wakeword_models=["hey_robot.tflite"])

while True:
    audio_chunk = stream.read(1600)  # 100ms @ 16kHz
    prediction = wake_model.predict(audio_chunk)

    if prediction["hey_robot"] > 0.7:  # Confidence threshold
        print("< Wake word detected! Listening for command...")
        process_voice_command()
```

**Popular wake word models:**
- Porcupine (Picovoice)
- Snowboy (offline)
- OpenWakeWord (open-source)

**Production:** Use custom wake word ("Humanoid", "Robot", company name) for uniqueness.
</details>

<details>
<summary>C) Faster Whisper model (Tiny instead of Base)</summary>

L **Wrong direction**

Noisy environments need MORE accuracy, not faster speed.

**Better:** Whisper Large (98% accuracy) to handle noisy audio, OR add noise suppression preprocessing.
</details>

<details>
<summary>D) Visual confirmation screen</summary>

 **Helpful but not most important**

Visual confirmation is good UX but doesn't prevent false activations from background noise.

**Wake word** is more important - prevents system from listening to non-user voices.

**Best practice:** Wake word + visual confirmation together.
</details>

---

## Score Interpretation

**Calculate your score:**
- 7/7 correct: < **Expert** - Ready to deploy voice-controlled robots
- 6/7 correct:  **Proficient** - Strong understanding
- 4-5/7 correct:  **Developing** - Review key optimization strategies
- 0-3/7 correct: = **Needs Review** - Re-read Chapter 11

---

## Key Takeaways

| Question | Topic | Review Section |
|----------|-------|----------------|
| Q1 | Whisper Model Selection | Stage 2: Speech Recognition |
| Q2 | Latency Bottlenecks | Pipeline Architecture  Latency Budget |
| Q3 | Error Handling | Testing & Evaluation |
| Q4 | Structured Parsing | Stage 3: Command Parsing with LLM |
| Q5 | Multimodal Requirements | Stage 4: VLA Action Generation |
| Q6 | Optimization Strategies | Pipeline Architecture |
| Q7 | Production Deployment | Testing & Evaluation |

---

## Next Steps

After completing this quiz:

1.  **Score &gt;= 80%:** Deploy complete voice-controlled robot!
2.  **Score &lt; 80%:** Review Chapter 11 sections
3. **Hands-on:** Run `code-examples/voice-pipeline/voice_robot.py`
4. **Production:** Add wake word detection, test in real environment

:::tip Final Challenge
**Build complete voice-controlled humanoid:**
1. Integrate Chapters 6 (Isaac Sim) + 10 (VLA) + 11 (Voice)
2. Deploy on RTX 4070 Ti with microphone
3. Test with 20 real voice commands
4. Achieve &lt;3 second end-to-end latency
5. Measure 80%+ success rate

**Success = You've built a working voice-controlled robot!**
:::
